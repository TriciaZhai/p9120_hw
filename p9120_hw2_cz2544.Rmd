---
title: "P9120_hw2_cz2544"
author: "Chunxiao Zhai"
date: "10/23/2019"
output: word_document
---

```{r setup, include=FALSE}
set.seed(2544)
library(splines)
library(tidyverse)
library(ggplot2)
library(MASS)
library(boot)
library(knitr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%")
```

3. (A simulation study).
(a) Generate a vector x consisting of 50 points drawn at random from Uniform[0, 1].
(b) Generate 100 training sets. Each training set consists of 50 pairs of (X, Y), with $(X_1, . . . , X_{50}) = \bf x$ and $Y_i = sin^3(2Ï€X_i^3) + \epsilon_i$ for i = 1, . . . , 50, where $\epsilon_i$ is drawn from the standard normal distribution. 

```{r dataset}
x = runif(50)
e = rnorm(50)
y = (sin(2*pi*x^3))^3 + e

# 100 training sets with same x and y with noise
x_100 =  matrix(rep(x,100), nrow = 50, byrow = FALSE)
e_100 = replicate(n = 100, 
          expr = rnorm(n = 50), 
          simplify = TRUE )
y_100 = (sin(2*pi*x_100^3))^3 + e_100
```

For each training set, do following.
i. Fit the data with methods/models listed below.
ii. Compute the vector of fitted value $\bf \hat y$ obtained from each method/model.
OLS with linear model: $\beta_0 + \beta_1X$.
```{r OLS_lm, echo=FALSE}
fit_1 = lm(y_100~x)
summary(fit_1)
y_hat_1 = fitted(fit_1)
```

OLS with cubic polynomial model: $\beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3$.
```{r OLS_cu, echo=FALSE}
fit_2 = lm(y_100 ~ x + I(x^2) + I(x^3))
summary(fit_2)
y_hat_2 = fitted(fit_2)
```

Cubic spline (or B-spline) with 2 knots at 0.33 and 0.66.
```{r Bsp}
#2 knots at 0.33 and 0.66
fit_3 = lm(y_100 ~ bs(x,knots = c(0.33,0.66)))
summary(fit_3)
y_hat_3 = fitted(fit_3)
```

Natural cubic spline with 5 knots at 0.1, 0.3, 0.5, 0.7, 0.9.
```{r Nsp}
#5 knots at 0.1, 0.3, 0.5, 0.7, 0.9
fit_4 = lm(y_100 ~ ns(x, ,knots = c(0.1, 0.3, 0.5, 0.7, 0.9)))
summary(fit_4)
y_hat_4 = fitted(fit_4)
```

Smoothing spline with tuning parameter chosen by GCV.
```{r Ssp}
#use cross-validation to choose best smoothing parameter
cv = rep_len(NA, 100)
y_hat_5 = matrix(NA, 50, 100)
for(i in 1:100) {
    tempfit = smooth.spline(x, y_100[,i], cv=TRUE)
    cv[i] = tempfit$cv.crit
    y_hat_5[,i] = fitted(tempfit)
}

# plot to check fit
# plot(x_100, y_hat_5)
# plot(x,y_100[,2],col='deepskyblue4',xlab='x')
```

(c) Now for each method/model, you obtain a matrix of fitted values, with the i-th row and j-th column value $\hat y_{ij}$ representing the fitted value at X = xi from the j-th training set.

(d) For each method/model, compute the pointwise variance of fitted values across the 100 training sets. This gives you a vector of pointwise variance. Plot the pointwise variance curves (against x) for each method/model. (Note: Your plot would be similar to Figure 5.3 of [ESL].)

```{r pointwise_varaince}
var_1 = rep(NA, 50)
var_2 = rep(NA, 50)
var_3 = rep(NA, 50)
var_4 = rep(NA, 50)
var_5 = rep(NA, 50)

for(i in 1:50) {
  var_1[i] = var(y_hat_1[i, ])
  var_2[i] = var(y_hat_2[i, ])
  var_3[i] = var(y_hat_3[i, ])
  var_4[i] = var(y_hat_4[i, ])
  var_5[i] = var(y_hat_5[i, ])
}

var_df = data.frame(cbind(x, var_1, var_2, var_3, var_4, var_5))

var_df_long = var_df %>% pivot_longer(
     var_1: var_5,
    names_to = "mdl_number", 
    values_to = "var")

ggplot(var_df_long,aes(x=x, y=var, col = mdl_number ))+geom_line()
```

4. The South African heart disease data on page 122 of the textbook. This data set can be found on the text book web site: https://web.stanford.edu/~hastie/ElemStatLearn/data.html
Divide the dataset into a training set consisting of the first 300 observations, and a test set consisting of the remaining observations. 
```{r heart_data}
heart = read.table("SAheart.data", sep = ",", header = TRUE)
index = as.numeric(row.names(heart)) # have to convert to numeric or -set will not work
train = sample(index, 300)
heart_train = heart[train, ]
heart_test = heart[-train, ]
```

Apply logistic regression, LDA and QDA on the training set. For each method, report the test error and its standard error over the test set. Briefly discuss your results.
```{r logistic}
names(heart)
logi.fit = glm(chd ~ sbp+tobacco+ldl+adiposity+famhist+typea+obesity+alcohol+age, 
               data = heart_train, family = binomial)
summary(logi.fit)
# stepwise remove adiposity, alcohol, obesity, sbp  for large p value
logi.fit = glm(chd ~ tobacco+ldl+famhist+typea+age, 
               data = heart_train, family = binomial)
summary(logi.fit)

test.logi = predict(logi.fit, newdata = heart_test ,type = "response", se.fit = TRUE)
test.logi.pred = ifelse(test.logi$fit > 0.5, 1, 0)
test.error.logi = mean( heart_test$chd != test.logi.pred)  # 47/162 = 0.2530864


# bootstrap for clasification CI
clserr.logi.fun <- function(mdl, data, indices) {
  d <- data[indices,] # allows boot to select sample
  pred.p = predict(mdl, newdata = d ,type = "response", se.fit = TRUE)
  pred.y = ifelse(pred.p$fit > 0.5, 1, 0)
  return(mean( d$chd != pred.y))
}
result1 = 
  boot(data=heart_test, statistic = clserr.logi.fun, R=1000, mdl= logi.fit)
# view result
# plot(result1)
result1  #std. error = 0.03476296
boot.ci(result1, type=c("basic","percent","bca"))
```


```{r LDA}
lda.fit = lda(chd ~., data = heart_train )
lda.fit

test.lda = predict(lda.fit, newdata = heart_test)
test.lda.pred = test.lda$class
test.error.lda = mean( heart_test$chd != test.lda.pred)  # 47/162

# bootstrap for classification error rate s.e.
clserr.da.fun <- function(mdl, data, indices) {
  d <- data[indices,] # allows boot to select sample
  pred = predict(mdl, newdata = d )$class
  return(mean( d$chd != pred))
}
result2 = 
  boot(data=heart_test, statistic = clserr.da.fun, R=1000, mdl= lda.fit)
# view result
# plot(result2)
result2  #std. error = 0.03418899
boot.ci(result2, type=c("basic","percent","bca"))
```


```{r QDA}
qda.fit = qda(chd ~., data = heart_train )
qda.fit

test.qda = predict(qda.fit, newdata = heart_test)
test.qda.pred = test.qda$class
test.error.qda = mean( heart_test$chd != test.qda.pred)  # 51/162

# bootstrap for classification error rate s.e.

result3 = 
  boot(data=heart_test, statistic = clserr.da.fun, R=1000, mdl= qda.fit)
# view result
# plot(result3)
result3  #std. error = 0.03511872
boot.ci(result3, type=c("basic","percent","bca"))
```

```{r tabble_results}
result = c(result1$t0, result2$t0, result2$t0)
result.se = c(sd(result1$t),sd(result2$t),sd(result3$t))
result.mtx = matrix(cbind(result,result.se), ncol = 2)
row.names(result.mtx) = c("logistic regression", "lda", "qda")
colnames(result.mtx) = c("cliassification error rate", "cliassification error rate SE")
kable(result.mtx, row.names = TRUE, col.names = TRUE)
```

